{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import timedelta\n",
    "from logging import INFO, basicConfig, info\n",
    "from time import time\n",
    "\n",
    "from numpy import digitize, percentile, where\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from torch import device as Device\n",
    "from torch.backends.mps import is_available as mps_available\n",
    "from torch.cuda import is_available as cuda_available\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# from segmentation_models_pytorch import Unet\n",
    "from src.models.unet import Unet\n",
    "from src.utils.dataset import ForestDataset\n",
    "from src.utils.loss import loss\n",
    "from src.utils.models import evaluation, load, save, training\n",
    "from src.utils.plots import plot_image_and_prediction\n",
    "from src.utils.predictions import predict_image\n",
    "from src.utils.preprocessing import Preprocessor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using mps device\n"
     ]
    }
   ],
   "source": [
    "patch_size = 128\n",
    "img_dir = \"data/images\"\n",
    "log_dir = \"logs\"\n",
    "model_dir = \"models\"\n",
    "patch_dir = f\"data/patches/{patch_size}\"\n",
    "patches_file = f\"data/patches/{patch_size}/info.fth\"\n",
    "gedi_file = \"data/gedi/gedi_complete.fth\"\n",
    "seed = 42\n",
    "batch_size = 16\n",
    "num_workers = 6\n",
    "learning_rate = 1e-4\n",
    "epochs = 5\n",
    "ranges = [(i, i + 5) for i in range(0, 50, 5)]\n",
    "device = Device(\"cuda\" if cuda_available() else \"mps\" if mps_available() else \"cpu\")\n",
    "\n",
    "basicConfig(level=INFO, filename=os.path.join(log_dir, \"main.log\"), filemode=\"w\")\n",
    "\n",
    "info(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess labels and patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting preprocessing...\n",
      "INFO:root:Directories validated.\n",
      "INFO:root:Images loaded.\n",
      "INFO:root:Number of images: 48\n",
      "INFO:root:Loaded existing patch info file. Skipping image processing.\n",
      "INFO:root:Total number of patches: 16339\n"
     ]
    }
   ],
   "source": [
    "# Create preprocessor\n",
    "preprocessor = Preprocessor(patches_file, img_dir, patch_dir, gedi_file, patch_size)\n",
    "\n",
    "preprocessor.run()\n",
    "\n",
    "# Extract patches\n",
    "patches = preprocessor.patches.sample(frac=0.5, random_state=seed)\n",
    "\n",
    "info(f\"Total number of patches: {len(patches)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets & dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create quantiles and stratify\n",
    "quantiles = percentile(patches.n_labels, [25, 50, 75])\n",
    "stratify = digitize(patches.n_labels, quantiles)\n",
    "\n",
    "# Split patches\n",
    "train, rest = split(patches, test_size=0.3, random_state=seed, stratify=stratify)\n",
    "\n",
    "stratify = stratify[patches.index.isin(rest.index)]\n",
    "\n",
    "val, test = split(rest, test_size=0.5, random_state=seed, stratify=stratify)\n",
    "\n",
    "# Create datasets\n",
    "train_data = ForestDataset(train, patch_dir)\n",
    "val_data = ForestDataset(val, patch_dir)\n",
    "test_data = ForestDataset(test, patch_dir)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size, True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_data, batch_size, False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_data, batch_size, False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Unet(\n",
      "  (down_conv1): ConvBlock(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (down_conv2): ConvBlock(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (down_conv3): ConvBlock(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (down_conv4): ConvBlock(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (down_conv5): ConvBlock(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (up_conv1): UpConvBlock(\n",
      "    (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv): ConvBlock(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up_conv2): UpConvBlock(\n",
      "    (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv): ConvBlock(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up_conv3): UpConvBlock(\n",
      "    (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv): ConvBlock(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up_conv4): UpConvBlock(\n",
      "    (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv): ConvBlock(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = Unet(\n",
    "    # encoder_name=\"inceptionv4\",\n",
    "    # encoder_weights=None,\n",
    "    # in_channels=4,\n",
    ").to(device)\n",
    "\n",
    "info(model)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Adam(model.parameters(), learning_rate)\n",
    "\n",
    "# Create writer\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "start = time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    training(train_loader, model, loss, device, writer, epoch, optimizer)\n",
    "    evaluation(val_loader, model, loss, device, writer, epoch)\n",
    "\n",
    "end = time()\n",
    "\n",
    "writer.close()\n",
    "\n",
    "info(f\"Training completed in {timedelta(seconds=(end - start))}\")\n",
    "\n",
    "score = evaluation(test_loader, model, loss, device)\n",
    "\n",
    "info(f\"Final loss on test set: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export/Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if epochs > 0:\n",
    "    info(f\"Saving model {model.name}\")\n",
    "    save(model, os.path.join(model_dir, f\"{model.name}.pt\"))\n",
    "else:\n",
    "    info(f\"Loading model {model.name}\")\n",
    "    model = load(model, os.path.join(model_dir, f\"{model.name}.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, prediction = predict_image(\n",
    "    model, device, \"data/images/L15-1059E-1348N.tif\", patch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = where(prediction < 5000, prediction, 0)\n",
    "\n",
    "plot_image_and_prediction(image, pred, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
