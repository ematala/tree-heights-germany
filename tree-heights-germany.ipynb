{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from logging import INFO, basicConfig, info\n",
    "from time import time\n",
    "\n",
    "from numpy import argmax\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from torch import device as Device\n",
    "from torch.backends.mps import is_available as mps_available\n",
    "from torch.cuda import is_available as cuda_available\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# from segmentation_models_pytorch import Unet\n",
    "# from src.models.vitnet import VitNet\n",
    "from src.models.unet import Unet\n",
    "from src.utils.dataset import ForestDataset\n",
    "from src.utils.loss import loss\n",
    "from src.utils.misc import seed\n",
    "from src.utils.models import evaluation, load, save, training\n",
    "from src.utils.plots import plot_image_and_prediction\n",
    "from src.utils.predictions import predict_image, predict_patch\n",
    "from src.utils.preprocessing import Preprocessor\n",
    "from src.utils.sampling import compute_sampling_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 256\n",
    "img_dir = \"data/images\"\n",
    "log_dir = \"logs\"\n",
    "model_dir = \"models\"\n",
    "patch_dir = f\"data/patches/{patch_size}\"\n",
    "patches_file = f\"data/patches/{patch_size}/info.fth\"\n",
    "gedi_file = \"data/gedi/gedi_complete.fth\"\n",
    "random_state = 42\n",
    "batch_size = 12\n",
    "num_workers = 6\n",
    "learning_rate = 1e-2\n",
    "epochs = 2\n",
    "is_training = True\n",
    "bins = list(range(0, 55, 5))\n",
    "device = Device(\"cuda\" if cuda_available() else \"mps\" if mps_available() else \"cpu\")\n",
    "\n",
    "seed(random_state)\n",
    "\n",
    "basicConfig(level=INFO)\n",
    "\n",
    "info(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess labels and patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor\n",
    "preprocessor = Preprocessor(patches_file, img_dir, patch_dir, gedi_file, patch_size)\n",
    "\n",
    "# Run preprocessor\n",
    "preprocessor.run()\n",
    "\n",
    "# Get labels\n",
    "labels = preprocessor.gedi.rh98\n",
    "\n",
    "# Get patches\n",
    "patches = preprocessor.patches\n",
    "\n",
    "info(f\"Total number of patches: {len(patches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of labels\n",
    "labels.hist(bins=bins)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datasets & dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stratification\n",
    "stratify = patches.bins.apply(argmax)\n",
    "\n",
    "# Split patches\n",
    "train, rest = split(\n",
    "    patches, test_size=0.3, random_state=random_state, stratify=stratify\n",
    ")\n",
    "\n",
    "# Create stratification for rest\n",
    "stratify = stratify[patches.index.isin(rest.index)]\n",
    "\n",
    "# Split rest\n",
    "val, test = split(rest, test_size=0.5, random_state=random_state, stratify=stratify)\n",
    "\n",
    "# Create datasets\n",
    "traindata = ForestDataset(train, patch_dir)\n",
    "valdata = ForestDataset(val, patch_dir)\n",
    "testdata = ForestDataset(test, patch_dir)\n",
    "\n",
    "# Create weighted sampler\n",
    "weights = compute_sampling_weights(train, labels, bins)\n",
    "sampler = WeightedRandomSampler(weights, len(train))\n",
    "\n",
    "# Create dataloaders\n",
    "trainloader = DataLoader(traindata, batch_size, False, sampler, num_workers=num_workers)\n",
    "valloader = DataLoader(valdata, batch_size, False, num_workers=num_workers)\n",
    "testloader = DataLoader(testdata, batch_size, False, num_workers=num_workers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create & Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VitNet().to(device)\n",
    "\n",
    "# Initialise model\n",
    "model = Unet().to(device)\n",
    "\n",
    "model_file = os.path.join(model_dir, f\"m-{model.name}-p{patch_size}-e{epochs}.pt\")\n",
    "\n",
    "info(model)\n",
    "\n",
    "if is_training:\n",
    "    # Create optimizer\n",
    "    optimizer = Adam(model.parameters(), learning_rate)\n",
    "\n",
    "    # Create scheduler\n",
    "    scheduler = CosineAnnealingLR(optimizer, epochs)\n",
    "\n",
    "    # Create writer\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    # images, _ = next(iter(trainloader))\n",
    "\n",
    "    # writer.add_images(\"images\", images[:, :3, :, :].to(device))\n",
    "    # writer.add_graph(model, images.to(device))\n",
    "\n",
    "    start = time()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        training(trainloader, model, loss, device, writer, epoch, optimizer)\n",
    "        evaluation(valloader, model, loss, device, writer, epoch)\n",
    "        scheduler.step()\n",
    "\n",
    "    end = time()\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    info(f\"Training completed in {timedelta(seconds=(end - start))}\")\n",
    "\n",
    "    score = evaluation(testloader, model, loss, device)\n",
    "\n",
    "    info(f\"Final loss on test set: {score}\")\n",
    "\n",
    "    info(f\"Saving model {model.name}\")\n",
    "\n",
    "    # Save model\n",
    "    save(model, model_file)\n",
    "else:\n",
    "    info(f\"Loading model {model.name}\")\n",
    "\n",
    "    # Load model\n",
    "    model = load(\n",
    "        model,\n",
    "        model_file,\n",
    "        device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 42\n",
    "patch = testdata[idx]\n",
    "img, pred = predict_patch(model, patch, device)\n",
    "plot_image_and_prediction(img, pred, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, prediction = predict_image(\n",
    "    model, device, f\"{img_dir}/L15-1059E-1348N.tif\", patch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_and_prediction(image, prediction, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_prediction(prediction, os.path.join(\"results\", \"L15-1059E-1348N-P.tiff\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
